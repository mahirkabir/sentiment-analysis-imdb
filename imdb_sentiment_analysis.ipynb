{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9dba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83000ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv('IMDB-Dataset.csv')\n",
    "\n",
    "# Convert Sentiment to Binary Labels\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03982a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"used device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50eecac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 40000\n",
      "Testing data size: 5000\n",
      "Validation data size: 5000\n"
     ]
    }
   ],
   "source": [
    "# prompt: split the whole dataset and use 80% for training/fine-tuning, 10% for testing, and 10% for validation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and a temporary set (test + validation)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into testing and validation sets\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data size: {len(train_df)}\")\n",
    "print(f\"Testing data size: {len(test_df)}\")\n",
    "print(f\"Validation data size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d0a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# prompt: use BERT tokenizer to tokenize the dataset\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_dataset(df):\n",
    "  tokenized_reviews = tokenizer(\n",
    "      df['review'].tolist(),\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      max_length=512,\n",
    "      return_tensors='pt'\n",
    "  )\n",
    "  return tokenized_reviews\n",
    "\n",
    "train_tokenized = tokenize_dataset(train_df)\n",
    "test_tokenized = tokenize_dataset(test_df)\n",
    "val_tokenized = tokenize_dataset(val_df)\n",
    "\n",
    "print(train_tokenized.keys())\n",
    "# Now you have tokenized inputs for your training, testing, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0ee10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['sentiment'].tolist()\n",
    "val_labels = val_df['sentiment'].tolist()\n",
    "test_labels = test_df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d555512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to PyTorch tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9142bac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 512])\n",
      "torch.Size([40000, 512])\n",
      "torch.Size([40000])\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenized['input_ids'].shape)\n",
    "print(train_tokenized['attention_mask'].shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e0fb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create TensorDataset for training data\n",
    "train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create TensorDataset for validation data\n",
    "val_dataset = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'], val_labels)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create TensorDataset for test data\n",
    "test_dataset = TensorDataset(test_tokenized['input_ids'], test_tokenized['attention_mask'], test_labels)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1caa5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 17 22:35:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              24W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d92d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,  # I have two labels: positive and negative\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f8339ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Set up the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps = number of batches * number of epochs\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,  # Typically warm-up steps are set as a percentage of the total steps\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1719165d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  2500.    Elapsed: 0:00:19.\n",
      "  Batch 80  of  2500.    Elapsed: 0:00:38.\n",
      "  Batch 120  of  2500.    Elapsed: 0:00:56.\n",
      "  Batch 160  of  2500.    Elapsed: 0:01:15.\n",
      "  Batch 200  of  2500.    Elapsed: 0:01:33.\n",
      "  Batch 240  of  2500.    Elapsed: 0:01:52.\n",
      "  Batch 280  of  2500.    Elapsed: 0:02:10.\n",
      "  Batch 320  of  2500.    Elapsed: 0:02:29.\n",
      "  Batch 360  of  2500.    Elapsed: 0:02:47.\n",
      "  Batch 400  of  2500.    Elapsed: 0:03:06.\n",
      "  Batch 440  of  2500.    Elapsed: 0:03:24.\n",
      "  Batch 480  of  2500.    Elapsed: 0:03:43.\n",
      "  Batch 520  of  2500.    Elapsed: 0:04:01.\n",
      "  Batch 560  of  2500.    Elapsed: 0:04:20.\n",
      "  Batch 600  of  2500.    Elapsed: 0:04:38.\n",
      "  Batch 640  of  2500.    Elapsed: 0:04:57.\n",
      "  Batch 680  of  2500.    Elapsed: 0:05:15.\n",
      "  Batch 720  of  2500.    Elapsed: 0:05:34.\n",
      "  Batch 760  of  2500.    Elapsed: 0:05:52.\n",
      "  Batch 800  of  2500.    Elapsed: 0:06:11.\n",
      "  Batch 840  of  2500.    Elapsed: 0:06:29.\n",
      "  Batch 880  of  2500.    Elapsed: 0:06:48.\n",
      "  Batch 920  of  2500.    Elapsed: 0:07:06.\n",
      "  Batch 960  of  2500.    Elapsed: 0:07:25.\n",
      "  Batch 1000  of  2500.    Elapsed: 0:07:43.\n",
      "  Batch 1040  of  2500.    Elapsed: 0:08:02.\n",
      "  Batch 1080  of  2500.    Elapsed: 0:08:20.\n",
      "  Batch 1120  of  2500.    Elapsed: 0:08:39.\n",
      "  Batch 1160  of  2500.    Elapsed: 0:08:57.\n",
      "  Batch 1200  of  2500.    Elapsed: 0:09:16.\n",
      "  Batch 1240  of  2500.    Elapsed: 0:09:35.\n",
      "  Batch 1280  of  2500.    Elapsed: 0:09:53.\n",
      "  Batch 1320  of  2500.    Elapsed: 0:10:12.\n",
      "  Batch 1360  of  2500.    Elapsed: 0:10:30.\n",
      "  Batch 1400  of  2500.    Elapsed: 0:10:49.\n",
      "  Batch 1440  of  2500.    Elapsed: 0:11:07.\n",
      "  Batch 1480  of  2500.    Elapsed: 0:11:26.\n",
      "  Batch 1520  of  2500.    Elapsed: 0:11:44.\n",
      "  Batch 1560  of  2500.    Elapsed: 0:12:03.\n",
      "  Batch 1600  of  2500.    Elapsed: 0:12:21.\n",
      "  Batch 1640  of  2500.    Elapsed: 0:12:40.\n",
      "  Batch 1680  of  2500.    Elapsed: 0:12:58.\n",
      "  Batch 1720  of  2500.    Elapsed: 0:13:17.\n",
      "  Batch 1760  of  2500.    Elapsed: 0:13:35.\n",
      "  Batch 1800  of  2500.    Elapsed: 0:13:54.\n",
      "  Batch 1840  of  2500.    Elapsed: 0:14:12.\n",
      "  Batch 1880  of  2500.    Elapsed: 0:14:31.\n",
      "  Batch 1920  of  2500.    Elapsed: 0:14:49.\n",
      "  Batch 1960  of  2500.    Elapsed: 0:15:08.\n",
      "  Batch 2000  of  2500.    Elapsed: 0:15:26.\n",
      "  Batch 2040  of  2500.    Elapsed: 0:15:45.\n",
      "  Batch 2080  of  2500.    Elapsed: 0:16:04.\n",
      "  Batch 2120  of  2500.    Elapsed: 0:16:22.\n",
      "  Batch 2160  of  2500.    Elapsed: 0:16:41.\n",
      "  Batch 2200  of  2500.    Elapsed: 0:16:59.\n",
      "  Batch 2240  of  2500.    Elapsed: 0:17:18.\n",
      "  Batch 2280  of  2500.    Elapsed: 0:17:36.\n",
      "  Batch 2320  of  2500.    Elapsed: 0:17:55.\n",
      "  Batch 2360  of  2500.    Elapsed: 0:18:13.\n",
      "  Batch 2400  of  2500.    Elapsed: 0:18:32.\n",
      "  Batch 2440  of  2500.    Elapsed: 0:18:50.\n",
      "  Batch 2480  of  2500.    Elapsed: 0:19:09.\n",
      "Average Training Loss: 0.2298366570694372\n",
      "Training Epoch took: 0:19:18\n",
      "\n",
      "Running Validation...\n",
      "Validation Loss: 0.1816465545775958\n",
      "Validation Accuracy: 0.94\n",
      "Validation took: 0:00:44\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  2500.    Elapsed: 0:00:18.\n",
      "  Batch 80  of  2500.    Elapsed: 0:00:37.\n",
      "  Batch 120  of  2500.    Elapsed: 0:00:55.\n",
      "  Batch 160  of  2500.    Elapsed: 0:01:14.\n",
      "  Batch 200  of  2500.    Elapsed: 0:01:32.\n",
      "  Batch 240  of  2500.    Elapsed: 0:01:51.\n",
      "  Batch 280  of  2500.    Elapsed: 0:02:09.\n",
      "  Batch 320  of  2500.    Elapsed: 0:02:28.\n",
      "  Batch 360  of  2500.    Elapsed: 0:02:46.\n",
      "  Batch 400  of  2500.    Elapsed: 0:03:05.\n",
      "  Batch 440  of  2500.    Elapsed: 0:03:23.\n",
      "  Batch 480  of  2500.    Elapsed: 0:03:42.\n",
      "  Batch 520  of  2500.    Elapsed: 0:04:00.\n",
      "  Batch 560  of  2500.    Elapsed: 0:04:19.\n",
      "  Batch 600  of  2500.    Elapsed: 0:04:37.\n",
      "  Batch 640  of  2500.    Elapsed: 0:04:56.\n",
      "  Batch 680  of  2500.    Elapsed: 0:05:14.\n",
      "  Batch 720  of  2500.    Elapsed: 0:05:33.\n",
      "  Batch 760  of  2500.    Elapsed: 0:05:51.\n",
      "  Batch 800  of  2500.    Elapsed: 0:06:10.\n",
      "  Batch 840  of  2500.    Elapsed: 0:06:28.\n",
      "  Batch 880  of  2500.    Elapsed: 0:06:47.\n",
      "  Batch 920  of  2500.    Elapsed: 0:07:05.\n",
      "  Batch 960  of  2500.    Elapsed: 0:07:24.\n",
      "  Batch 1000  of  2500.    Elapsed: 0:07:42.\n",
      "  Batch 1040  of  2500.    Elapsed: 0:08:01.\n",
      "  Batch 1080  of  2500.    Elapsed: 0:08:19.\n",
      "  Batch 1120  of  2500.    Elapsed: 0:08:38.\n",
      "  Batch 1160  of  2500.    Elapsed: 0:08:56.\n",
      "  Batch 1200  of  2500.    Elapsed: 0:09:15.\n",
      "  Batch 1240  of  2500.    Elapsed: 0:09:33.\n",
      "  Batch 1280  of  2500.    Elapsed: 0:09:52.\n",
      "  Batch 1320  of  2500.    Elapsed: 0:10:10.\n",
      "  Batch 1360  of  2500.    Elapsed: 0:10:29.\n",
      "  Batch 1400  of  2500.    Elapsed: 0:10:48.\n",
      "  Batch 1440  of  2500.    Elapsed: 0:11:06.\n",
      "  Batch 1480  of  2500.    Elapsed: 0:11:25.\n",
      "  Batch 1520  of  2500.    Elapsed: 0:11:43.\n",
      "  Batch 1560  of  2500.    Elapsed: 0:12:02.\n",
      "  Batch 1600  of  2500.    Elapsed: 0:12:20.\n",
      "  Batch 1640  of  2500.    Elapsed: 0:12:39.\n",
      "  Batch 1680  of  2500.    Elapsed: 0:12:57.\n",
      "  Batch 1720  of  2500.    Elapsed: 0:13:16.\n",
      "  Batch 1760  of  2500.    Elapsed: 0:13:34.\n",
      "  Batch 1800  of  2500.    Elapsed: 0:13:53.\n",
      "  Batch 1840  of  2500.    Elapsed: 0:14:11.\n",
      "  Batch 1880  of  2500.    Elapsed: 0:14:30.\n",
      "  Batch 1920  of  2500.    Elapsed: 0:14:48.\n",
      "  Batch 1960  of  2500.    Elapsed: 0:15:07.\n",
      "  Batch 2000  of  2500.    Elapsed: 0:15:25.\n",
      "  Batch 2040  of  2500.    Elapsed: 0:15:44.\n",
      "  Batch 2080  of  2500.    Elapsed: 0:16:02.\n",
      "  Batch 2120  of  2500.    Elapsed: 0:16:21.\n",
      "  Batch 2160  of  2500.    Elapsed: 0:16:39.\n",
      "  Batch 2200  of  2500.    Elapsed: 0:16:58.\n",
      "  Batch 2240  of  2500.    Elapsed: 0:17:16.\n",
      "  Batch 2280  of  2500.    Elapsed: 0:17:35.\n",
      "  Batch 2320  of  2500.    Elapsed: 0:17:53.\n",
      "  Batch 2360  of  2500.    Elapsed: 0:18:12.\n",
      "  Batch 2400  of  2500.    Elapsed: 0:18:30.\n",
      "  Batch 2440  of  2500.    Elapsed: 0:18:49.\n",
      "  Batch 2480  of  2500.    Elapsed: 0:19:07.\n",
      "Average Training Loss: 0.1275881808409933\n",
      "Training Epoch took: 0:19:17\n",
      "\n",
      "Running Validation...\n",
      "Validation Loss: 0.24591720875013418\n",
      "Validation Accuracy: 0.94\n",
      "Validation took: 0:00:44\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  2500.    Elapsed: 0:00:18.\n",
      "  Batch 80  of  2500.    Elapsed: 0:00:37.\n",
      "  Batch 120  of  2500.    Elapsed: 0:00:55.\n",
      "  Batch 160  of  2500.    Elapsed: 0:01:14.\n",
      "  Batch 200  of  2500.    Elapsed: 0:01:32.\n",
      "  Batch 240  of  2500.    Elapsed: 0:01:51.\n",
      "  Batch 280  of  2500.    Elapsed: 0:02:09.\n",
      "  Batch 320  of  2500.    Elapsed: 0:02:28.\n",
      "  Batch 360  of  2500.    Elapsed: 0:02:46.\n",
      "  Batch 400  of  2500.    Elapsed: 0:03:05.\n",
      "  Batch 440  of  2500.    Elapsed: 0:03:23.\n",
      "  Batch 480  of  2500.    Elapsed: 0:03:42.\n",
      "  Batch 520  of  2500.    Elapsed: 0:04:00.\n",
      "  Batch 560  of  2500.    Elapsed: 0:04:19.\n",
      "  Batch 600  of  2500.    Elapsed: 0:04:37.\n",
      "  Batch 640  of  2500.    Elapsed: 0:04:56.\n",
      "  Batch 680  of  2500.    Elapsed: 0:05:14.\n",
      "  Batch 720  of  2500.    Elapsed: 0:05:33.\n",
      "  Batch 760  of  2500.    Elapsed: 0:05:51.\n",
      "  Batch 800  of  2500.    Elapsed: 0:06:10.\n",
      "  Batch 840  of  2500.    Elapsed: 0:06:28.\n",
      "  Batch 880  of  2500.    Elapsed: 0:06:47.\n",
      "  Batch 920  of  2500.    Elapsed: 0:07:05.\n",
      "  Batch 960  of  2500.    Elapsed: 0:07:24.\n",
      "  Batch 1000  of  2500.    Elapsed: 0:07:42.\n",
      "  Batch 1040  of  2500.    Elapsed: 0:08:01.\n",
      "  Batch 1080  of  2500.    Elapsed: 0:08:19.\n",
      "  Batch 1120  of  2500.    Elapsed: 0:08:38.\n",
      "  Batch 1160  of  2500.    Elapsed: 0:08:56.\n",
      "  Batch 1200  of  2500.    Elapsed: 0:09:15.\n",
      "  Batch 1240  of  2500.    Elapsed: 0:09:34.\n",
      "  Batch 1280  of  2500.    Elapsed: 0:09:52.\n",
      "  Batch 1320  of  2500.    Elapsed: 0:10:11.\n",
      "  Batch 1360  of  2500.    Elapsed: 0:10:29.\n",
      "  Batch 1400  of  2500.    Elapsed: 0:10:48.\n",
      "  Batch 1440  of  2500.    Elapsed: 0:11:06.\n",
      "  Batch 1480  of  2500.    Elapsed: 0:11:25.\n",
      "  Batch 1520  of  2500.    Elapsed: 0:11:43.\n",
      "  Batch 1560  of  2500.    Elapsed: 0:12:02.\n",
      "  Batch 1600  of  2500.    Elapsed: 0:12:20.\n",
      "  Batch 1640  of  2500.    Elapsed: 0:12:39.\n",
      "  Batch 1680  of  2500.    Elapsed: 0:12:57.\n",
      "  Batch 1720  of  2500.    Elapsed: 0:13:16.\n",
      "  Batch 1760  of  2500.    Elapsed: 0:13:34.\n",
      "  Batch 1800  of  2500.    Elapsed: 0:13:53.\n",
      "  Batch 1840  of  2500.    Elapsed: 0:14:11.\n",
      "  Batch 1880  of  2500.    Elapsed: 0:14:30.\n",
      "  Batch 1920  of  2500.    Elapsed: 0:14:48.\n",
      "  Batch 1960  of  2500.    Elapsed: 0:15:07.\n",
      "  Batch 2000  of  2500.    Elapsed: 0:15:25.\n",
      "  Batch 2040  of  2500.    Elapsed: 0:15:44.\n",
      "  Batch 2080  of  2500.    Elapsed: 0:16:02.\n",
      "  Batch 2120  of  2500.    Elapsed: 0:16:21.\n",
      "  Batch 2160  of  2500.    Elapsed: 0:16:39.\n",
      "  Batch 2200  of  2500.    Elapsed: 0:16:58.\n",
      "  Batch 2240  of  2500.    Elapsed: 0:17:16.\n",
      "  Batch 2280  of  2500.    Elapsed: 0:17:35.\n",
      "  Batch 2320  of  2500.    Elapsed: 0:17:53.\n",
      "  Batch 2360  of  2500.    Elapsed: 0:18:12.\n",
      "  Batch 2400  of  2500.    Elapsed: 0:18:30.\n",
      "  Batch 2440  of  2500.    Elapsed: 0:18:49.\n",
      "  Batch 2480  of  2500.    Elapsed: 0:19:07.\n",
      "Average Training Loss: 0.06366873426203383\n",
      "Training Epoch took: 0:19:17\n",
      "\n",
      "Running Validation...\n",
      "Validation Loss: 0.25857173053851984\n",
      "Validation Accuracy: 0.94\n",
      "Validation took: 0:00:44\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Function to format elapsed time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n======== Epoch {epoch + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()  # Put model in training mode\n",
    "\n",
    "    # Iterate over each batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f'  Batch {step}  of  {len(train_dataloader)}.    Elapsed: {elapsed}.')\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()  # Zero any previously calculated gradients\n",
    "\n",
    "        # Perform a forward pass (calculate predictions)\n",
    "        outputs = model(input_ids=b_input_ids,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Accumulate the training loss for this batch\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to avoid \"exploding gradients\" problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "    print(f\"Training Epoch took: {training_time}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    print(\"\\nRunning Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    total_eval_accuracy = 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():  # Do not calculate gradients (to save memory and speed up validation)\n",
    "            outputs = model(input_ids=b_input_ids,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU to calculate accuracy\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        total_eval_accuracy += np.sum(preds == label_ids) / len(label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss}\")\n",
    "    print(f\"Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"Validation took: {validation_time}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e58bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Test Set Evaluation...\n",
      "Test Loss: 0.2392246308466851\n",
      "Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning Test Set Evaluation...\")\n",
    "\n",
    "model.eval()\n",
    "total_test_accuracy = 0\n",
    "total_test_loss = 0\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=b_input_ids,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "\n",
    "    total_test_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU to calculate accuracy\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    total_test_accuracy += np.sum(preds == label_ids) / len(label_ids)\n",
    "\n",
    "avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}\")\n",
    "print(f\"Test Accuracy: {avg_test_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
